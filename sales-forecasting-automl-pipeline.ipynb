{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bffc96-d9c5-4081-9f64-aec452e571c0",
   "metadata": {},
   "source": [
    "# Time Series Forecasting with Vertex Forecast & Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d28d30-cba6-4ce3-a7b5-57fea5365105",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This notebook shows how to use the components defined in [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) to build an automl forecasting model workflow on [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines).\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this example, you'll learn how to use components from `google_cloud_pipeline_components` to:\n",
    "- create a _Dataset_ using data in cloud storage\n",
    "- train an AutoML forecasting Model\n",
    "\n",
    "\n",
    "The components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform).\n",
    "\n",
    "In addition, you'll use the `kfp.v2.google.experimental.run_as_aiplatform_custom_job` method to train a custom model.\n",
    "\n",
    "The components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform).\n",
    "(From that page, see also the `CustomPythonPackageTrainingJobRunOp` and `CustomContainerTrainingJobRunOp` components, which similarly run 'custom' training, but as with the related `google.cloud.aiplatform.CustomContainerTrainingJob` and `google.cloud.aiplatform.CustomPythonPackageTrainingJob` methods from the [Vertex AI SDK](https://googleapis.dev/python/aiplatform/latest/aiplatform.html), also upload the trained model).\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Training and Serving\n",
    "* Cloud Storage\n",
    "* BigQuery Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e8692-e328-402d-9b58-c59e4cee5b9f",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "This notebook does not require a GPU runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43871b55-4cde-4138-81d0-639a383f5513",
   "metadata": {},
   "source": [
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12790e9d-159f-4067-8be5-7ba7781e810a",
   "metadata": {},
   "source": [
    "### Install additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718cf38-b836-43ae-84ac-da6df463cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New\n",
    "! pip3 install -U google-cloud-storage $USER_FLAG\n",
    "# ! pip3 install $USER kfp google-cloud-pipeline-components --upgrade\n",
    "!git clone https://github.com/kubeflow/pipelines.git\n",
    "!pip install pipelines/components/google-cloud/.\n",
    "!pip install google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c6a28-ebdd-437e-9ee8-d35bc155227a",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518ddbb-40e7-4ff5-993f-9b38fa4c7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab5b83-fc10-4223-a364-d81085243079",
   "metadata": {},
   "source": [
    "### Check the versions of the packages you installed.  The KFP SDK version should be >=1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e0df1-f068-4069-9ae4-4f04b480e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcac683-35bb-4b0b-9591-880c6d454b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed70b26-077a-41b5-9d0e-6bd75e310425",
   "metadata": {},
   "source": [
    "Ignore the cell below if you see the project id - Otherwise, set your project ID here below. Replace \"CHANGE_THIS\" to your project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2251c2-06cb-4179-9a35-3d42fe2a27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '<CHANGE_THIS>'  # <--- TODO: CHANGE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65853a4-f5cc-4ce6-a338-d1e902c0422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Project Configuration:\n",
    "# project where pipeline and vertex jobs are executed\n",
    "\n",
    "LOCATION = 'us-central1' # {type: 'string'} \n",
    "BQ_LOCATION = 'us-central1' # {type: 'string'}\n",
    "\n",
    "SCOPES = (\n",
    "  'https://www.googleapis.com/auth/cloud-platform',\n",
    ")\n",
    "\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={PROJECT_ID}\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9473dee-80cb-4a8b-ab32-a98bfac70952",
   "metadata": {},
   "source": [
    "### The next few steps will create a Cloud Storage bucket and copy the necessary files to the bucket. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16043b-312b-42e5-a8e3-c0130879bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUCKET_NAME = \"gs://project849089-bucket/vai\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"gs://\" + PROJECT_ID + \"_vertexai\"\n",
    "\n",
    "print(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd66fa9-bdb5-4d00-807b-8f2a9abc3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86725999-25a2-43b9-b56b-87c96444c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Pipeline Parameters\n",
    "#USER = 'skewalramani'  #  {type: 'string'} <--- TODO: CHANGE THIS\n",
    "#BUCKET_NAME = 'sk-forecasting'  #  {type: 'string'} <--- TODO: CHANGE THIS\n",
    "\n",
    "USER = '<CHANGE this>'  #  {type: 'string'} <--- TODO: CHANGE THIS\n",
    "GS_PIPELINE_ROOT_PATH = 'gs://{}/pipeline_root/{}'.format(BUCKET_NAME, USER)\n",
    "print('GS_PIPELINE_ROOT_PATH: {}'.format(GS_PIPELINE_ROOT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0374ec-2d7a-4bba-b353-496439818bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from google import auth\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.experimental import forecasting as gcc_aip_forecasting\n",
    "import google.cloud.aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "#from google.colab import auth as colab_auth\n",
    "#from google.colab import drive\n",
    "\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "\n",
    "from matplotlib import dates as mdates\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4baf5-fb67-4e9f-bb29-c7580ae87fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'aiplatform SDK version: {google.cloud.aiplatform.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbcb0e-ecd1-4949-a59a-0670edfb6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate_user()\n",
    "credentials, _ = auth.default()\n",
    "credentials, _ = auth.default(scopes=SCOPES, quota_project_id=PROJECT_ID)\n",
    "bq_client = bigquery.Client(project=PROJECT_ID, credentials=credentials)\n",
    "pipeline_client = pipelines_client.AIPlatformClient(\n",
    "  project_id=PROJECT_ID,\n",
    "  region=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3365ed6-c4e5-4e24-ac3c-bb1c36cef270",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINES = {}\n",
    "\n",
    "PIPELINES_FILEPATH = 'gs://sk-forecasting/pipelines/pipelines.json'\n",
    "\n",
    "if os.path.isfile(PIPELINES_FILEPATH):\n",
    "  with open(PIPELINES_FILEPATH) as f:\n",
    "    PIPELINES = json.load(f)\n",
    "else:\n",
    "  PIPELINES = {}\n",
    "\n",
    "def save_pipelines():\n",
    "  with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "    json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559286d-277f-445a-a1e2-ac2b998df408",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.v2.dsl.component(base_image='python:3.9')\n",
    "def create_input_table_specs(\n",
    "  sales_table_uri: str,\n",
    "  time_granularity_unit: str,\n",
    "  time_granularity_quantity: int,\n",
    ") -> NamedTuple(\n",
    "  'Output',\n",
    "  [('input_table_specs', str), ('model_feature_columns', str)],\n",
    "):\n",
    "  import json\n",
    "\n",
    "  sales_table_specs = {\n",
    "    'bigquery_uri': sales_table_uri,\n",
    "    'table_type': 'FORECASTING_PRIMARY',\n",
    "    'forecasting_primary_table_metadata': {\n",
    "        'time_column': 'ds',\n",
    "        'target_column': 'y',\n",
    "        'time_series_identifier_columns': ['xsku', 'xstore'],\n",
    "        'unavailable_at_forecast_columns': [],\n",
    "        'time_granularity': {\n",
    "          'unit': time_granularity_unit,\n",
    "          'quantity': time_granularity_quantity,\n",
    "        },\n",
    "        # 'predefined_splits_column': 'ml_use',\n",
    "        # 'predefined_split_column': 'ml_use', # model_override\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n",
    "  model_feature_columns = [\n",
    "    'cal_name',\n",
    "    'cal_val',\n",
    "    'discount_amount',\n",
    "    'ds',\n",
    "    'oss_Days',\n",
    "    'promo_type',\n",
    "    'wic',\n",
    "    'xclass',\n",
    "    'xcoupon_type',\n",
    "    'xdiscount_type',\n",
    "    'xsku',\n",
    "    'xstore',\n",
    "    'xstore_type',\n",
    "    'xsubclass',\n",
    "    'y'\n",
    "  ]\n",
    "\n",
    "  input_table_specs = [\n",
    "    sales_table_specs\n",
    "\n",
    "  ]\n",
    "\n",
    "  return (\n",
    "    json.dumps(input_table_specs),  # input_table_specs\n",
    "    json.dumps(model_feature_columns),  # model_feature_columns\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdde576-e37c-4845-8f79-76534cf1f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDE = 'True' # replace BQ eval tables?\n",
    "VERSION = 'sales_v1' # <--- TODO; Pipeline & model identifier;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed955d54-c7db-4f41-b1cc-99caebc88c4f",
   "metadata": {},
   "source": [
    "Then, [`google_cloud_pipeline_components`](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud) components are used to define the rest of the pipeline: upload the model, create an endpoint, and deploy the model to the endpoint. (While not shown in this example, the model deploy will create an endpoint if one is not provided.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45385b-1c0d-4380-aa7f-7440d8fc88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_TAG = 'train-sales-ds' # <--- TODO; optionally name pipeline\n",
    "@kfp.v2.dsl.pipeline(\n",
    "  name=f'{VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "  vertex_project: str,\n",
    "  location: str,\n",
    "  version: str,\n",
    "  data_source_dataset: str,\n",
    "  sales_table_uri: str,\n",
    "  time_granularity_unit: str,\n",
    "  time_granularity_quantity: int,\n",
    "  context_window: int,\n",
    "  forecast_horizon: int,\n",
    "  override: str,\n",
    "  budget_milli_node_hours: int = 16000,\n",
    "):\n",
    "\n",
    "\n",
    "  create_input_table_specs_op = create_input_table_specs(\n",
    "    sales_table_uri=sales_table_uri,\n",
    "    time_granularity_unit=time_granularity_unit,\n",
    "    time_granularity_quantity=time_granularity_quantity\n",
    "   )\n",
    "#  create_input_table_specs_op.after(create_dataset_op)\n",
    "\n",
    "  forecasting_validation_op = gcc_aip_forecasting.ForecastingValidationOp(\n",
    "    input_tables=create_input_table_specs_op.outputs['input_table_specs'],\n",
    "    validation_theme='FORECASTING_TRAINING',\n",
    "  )\n",
    "\n",
    "  forecasting_preprocessing_op = gcc_aip_forecasting.ForecastingPreprocessingOp(\n",
    "    project=vertex_project,\n",
    "    input_tables=create_input_table_specs_op.outputs['input_table_specs'],\n",
    "    preprocessing_bigquery_dataset=data_source_dataset,\n",
    "  )\n",
    "  forecasting_preprocessing_op.after(forecasting_validation_op)\n",
    "\n",
    "  \n",
    "  prepare_data_for_train_op = gcc_aip_forecasting.ForecastingPrepareDataForTrainOp(\n",
    "      input_tables=(\n",
    "          create_input_table_specs_op.outputs['input_table_specs']\n",
    "      ),\n",
    "      preprocess_metadata=(\n",
    "          forecasting_preprocessing_op.outputs['preprocess_metadata']\n",
    "      ),\n",
    "      model_feature_columns=(\n",
    "          create_input_table_specs_op.outputs['model_feature_columns']\n",
    "      ),\n",
    "    )\n",
    "\n",
    "\n",
    "  time_series_dataset_create_op = gcc_aip.TimeSeriesDatasetCreateOp(\n",
    "    display_name='sales_training_dataset', \n",
    "    bq_source=prepare_data_for_train_op.outputs['preprocess_bq_uri'],\n",
    "    project=vertex_project,\n",
    "    location=location,\n",
    "  )\n",
    "\n",
    "#  mape_model_version = f'{VERSION}-seq2seq-mape' # TODO: determines model display name and eval BQ table name # f'{VERSION}-l2l-mape'\n",
    "  rmse_model_version = f'{VERSION}-seq2seq-rmse' # TODO: determines model display name and eval BQ table name\n",
    "\n",
    "\n",
    "\n",
    "  rmse_model_op = gcc_aip_forecasting.ForecastingTrainingWithExperimentsOp(\n",
    "      display_name=f'train-{rmse_model_version}',\n",
    "      model_display_name=rmse_model_version,\n",
    "      model_labels={'model_override' : 'se2seq-hier'}, # model_override : se2seq-hier, tft\n",
    "      # model_labels={'model_type' : 'l2l'},\n",
    "      dataset=time_series_dataset_create_op.outputs['dataset'],\n",
    "      context_window=context_window,\n",
    "      forecast_horizon=forecast_horizon,\n",
    "      budget_milli_node_hours=budget_milli_node_hours,\n",
    "      project=vertex_project,\n",
    "      location=location,\n",
    "      export_evaluated_data_items=True,\n",
    "      #export_evaluated_data_items_bigquery_destination_uri=get_eval_dataset_path_uri_op.outputs['model_2_bigquery_table_uri'], # must be format: ``bq://<project_id>:<dataset_id>:<table>``\n",
    "      export_evaluated_data_items_override_destination=True,\n",
    "      target_column=prepare_data_for_train_op.outputs['target_column'],\n",
    "      time_column=prepare_data_for_train_op.outputs['time_column'],\n",
    "      time_series_identifier_column=prepare_data_for_train_op.outputs['time_series_identifier_column'],\n",
    "      time_series_attribute_columns=prepare_data_for_train_op.outputs['time_series_attribute_columns'],\n",
    "      unavailable_at_forecast_columns=prepare_data_for_train_op.outputs['unavailable_at_forecast_columns'],\n",
    "      available_at_forecast_columns=prepare_data_for_train_op.outputs['available_at_forecast_columns'],\n",
    "      data_granularity_unit=prepare_data_for_train_op.outputs['data_granularity_unit'],\n",
    "      data_granularity_count=prepare_data_for_train_op.outputs['data_granularity_count'],\n",
    "      predefined_split_column_name= '', # prepare_data_for_train_op.outputs['predefined_split_column'],\n",
    "      column_transformations=prepare_data_for_train_op.outputs['column_transformations'],\n",
    "      weight_column=prepare_data_for_train_op.outputs['weight_column'],\n",
    "      optimization_objective='minimize-rmse',\n",
    "      additional_experiments={\n",
    "          'forecasting_model_type_override': 'seq2seq',\n",
    "          'forecasting_hierarchical_group_column_names':'xksu, xstore'},\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ee395-2995-4e7f-b702-b9276414e8dc",
   "metadata": {},
   "source": [
    "## Compile and run the pipeline\n",
    "\n",
    "Now, you're ready to compile the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308bf8e-2621-4a9e-ba97-e20138d5bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='sales_fsct_pipeline_spec.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59e1af-e198-4d80-994d-6d0df2bcc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROJECT_ID = 'sk-ai-ml-poc' # <--- TODO: If not set\n",
    "PROJECT_ID = '<CHANGE THIS>' # <--- TODO: If not set\n",
    "LOCATION = 'us-central1' # <--- TODO: If not set\n",
    "location = 'us-central1' # <--- TODO: If not set\n",
    "bq_location = 'us-central1' # <--- TODO: If not set\n",
    "\n",
    "\n",
    "# BQ dataset for source data source\n",
    "DATA_SOURCE_DATASET = 'sales_train_data'\n",
    "\n",
    "\n",
    "# training BQ tables\n",
    "#SALES_TABLE = 'sk-ai-ml-poc.h_data_1.train_data_1'  #  {type: 'string'} <---TODO: CHANGE THIS\n",
    "SALES_TABLE = '<CHANGE THIS>' \n",
    "\n",
    "\n",
    "# TODO: Forecasting Configuration:\n",
    "HISTORY_WINDOW_n = 26 #  {type: 'integer'} # context_window\n",
    "FORECAST_HORIZON = 26 #  {type: 'integer'} \n",
    "BUDGET_MILLI_NODE_HOURS = 1000\n",
    "\n",
    "\n",
    "\n",
    "assert HISTORY_WINDOW_n, 'the value for this variable must be set'\n",
    "assert FORECAST_HORIZON, 'the value for this variable must be set'\n",
    "assert LOCATION, 'the value for this variable must be set'\n",
    "assert PROJECT_ID, 'the value for this variable must be set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c960d-ff94-45d4-9e53-6ced9fd7b3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True # True creates new pipeline instance for execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00be1e8-7c29-4a9e-ac2b-3b62cd69c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PIPELINES.get('train') or overwrite:\n",
    "  response = pipeline_client.create_run_from_job_spec(\n",
    "    job_spec_path='sales_fsct_pipeline_spec.json',\n",
    "    # service_account=SERVICE_ACCOUNT, # <--- TODO: Uncomment if needed\n",
    "    parameter_values={\n",
    "      'vertex_project': PROJECT_ID,\n",
    "      'location': LOCATION,\n",
    "      'version': VERSION,\n",
    "      'data_source_dataset': DATA_SOURCE_DATASET,\n",
    "      'sales_table_uri': f'bq://{SALES_TABLE}',  \n",
    "      'time_granularity_unit': 'WEEK',\n",
    "      'time_granularity_quantity': 1,\n",
    "      'context_window': HISTORY_WINDOW_n,\n",
    "      'forecast_horizon': FORECAST_HORIZON,\n",
    "      'override': OVERRIDE,\n",
    "      'budget_milli_node_hours': BUDGET_MILLI_NODE_HOURS,\n",
    "    },\n",
    "    pipeline_root=f'{GS_PIPELINE_ROOT_PATH}/{VERSION}',\n",
    "  )\n",
    "  PIPELINES['train'] = response['name']\n",
    "  # save_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e36f10-23e0-4741-b369-67819428ee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
